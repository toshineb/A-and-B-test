# ğŸ§ª A/B Testing for Data-Driven Decision Making

## ğŸ“Œ Project Overview

This project presents a robust **A/B testing analysis** using Python to determine the impact of changes in a product, marketing, or user interface. It simulates a controlled experiment where users are split into two groups **Group A (Control)** and **Group B (Variant)** to test whether a new design, feature, or strategy improves key performance metrics such as **conversion rate, click-through rate (CTR), or revenue per user**.

> ğŸ”¬ Designed for data-backed decisions, this notebook follows the best practices in experimental design, hypothesis testing, and effect size interpretation.

---

## ğŸ¯ Problem Statement

> ğŸ’¡ "Does the new version of a product or webpage outperform the original in terms of user engagement and conversion?"

By using A/B testing principles, this project answers critical business questions with statistical confidence, eliminating guesswork from product decisions.

---

## ğŸ“Š What This Project Covers

- âœ… Hypothesis Formulation (Null vs Alternative)
- ğŸ“Š Exploratory Data Analysis of both groups
- ğŸ“ Normality Testing (Shapiro-Wilk, Histograms)
- ğŸ§ª Statistical Tests (t-test, Welchâ€™s t-test)
- ğŸ” p-value interpretation and significance thresholding
- ğŸ§  Actionable Recommendation based on effect size

---

## ğŸ” Key Insights

- The average conversion rate for **Group B** exceeded that of **Group A** by a statistically significant margin (p < 0.05).
- Distributions were assessed for normality; appropriate test (e.g., Welchâ€™s t-test) was selected based on variance assumptions.
- The confidence interval shows practical significance, suggesting the treatment is **not only statistically significant but also impactful.**

> ğŸ“ˆ This kind of analysis is crucial for **growth teams, marketers, and product managers** aiming to drive real change through experimentation.

---

## ğŸ›  Tools & Technologies Used

- **Python**  
- **pandas** â€“ Data manipulation  
- **scipy.stats** â€“ Hypothesis testing  
- **matplotlib, seaborn** â€“ Visual comparison of group metrics  
- **Jupyter Notebook** â€“ Workflow and presentation

---

## âœ… Skills Demonstrated

- ğŸ§ª Experimental Design (Randomized Control Trials)  
- ğŸ“ˆ Statistical Hypothesis Testing  
- ğŸ“Š Visual Data Storytelling  
- ğŸ” p-value, confidence intervals, effect size interpretation  
- ğŸ’¡ Business Insight Communication  

---

## ğŸ‘¨â€ğŸ’¼ About Me

Iâ€™m **Tosin Bello**, a **Data Analyst and Data Scientist** passionate about using data to optimize user experience, validate product decisions, and measure impact scientifically. This A/B testing project reflects my ability to **bridge the gap between data science and business strategy**.

ğŸ“¬ **Letâ€™s connect**:  
- **LinkedIn**: [Tosin Bello](https://www.linkedin.com/in/tosinbellofin)  
- **Email**: toshineb@email.com

---

## ğŸ§ª How to Reproduce

1. Clone this repo  
2. Open `A_and_B_test.ipynb` in Jupyter Notebook  
3. Run the cells step-by-step to explore the experimental design, data exploration, and conclusion logic  
4. Modify metrics or groups to simulate new A/B testing scenarios

---

## â­ Like this project? Star the repo and follow for more work on experimentation, product analytics, and data-driven decision making.
